{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 Sparse Spam Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of an app to classify phone SMS messages as either \"spam\" or \"ham\" (=not spam).  Some of this content has been adapted from a tutorial by Radimre Hurek:  https://radimrehurek.com/data_science_python/ and has been updated by Dr. Riley.  \n",
    "\n",
    "Please follow through this notebook linearly and insert your modifications and additions appropriately throughout.  You will also need to update some of the existing cells to conform to the style expectations of the checklist.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets start with importing some things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ad.msoe.edu/singkhamj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ad.msoe.edu/singkhamj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ad.msoe.edu/singkhamj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import nltk\n",
    "import sys\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load data, explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the dataset and put it in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains **a collection of more than 5 thousand SMS phone messages** (see the `readme` file for more info).  First, load them using Pandas with one column named `label` and one named `message`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "6      ham  Even my brother is not like to speak with me. ...\n",
      "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "8     spam  WINNER!! As a valued network customer you have...\n",
      "9     spam  Had your mobile 11 months or more? U R entitle...\n",
      "10     ham  I'm gonna be home soon and i don't want to tal...\n",
      "11    spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
      "12    spam  URGENT! You have won a 1 week FREE membership ...\n",
      "13     ham  I've been searching for the right words to tha...\n",
      "14     ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "15    spam  XXXMobileMovieClub: To use your credit, click ...\n",
      "16     ham                         Oh k...i'm watching here:)\n",
      "17     ham  Eh u remember how 2 spell his name... Yes i di...\n",
      "18     ham  Fine if thats the way u feel. Thats the way ...\n",
      "19    spam  England v Macedonia - dont miss the goals/team...\n",
      "20     ham          Is that seriously how you spell his name?\n",
      "21     ham    I‘m going to try for 2 months ha ha only joking\n",
      "22     ham  So ü pay first lar... Then when is da stock co...\n",
      "23     ham  Aft i finish my lunch then i go str down lor. ...\n",
      "24     ham  Ffffffffff. Alright no way I can meet up with ...\n",
      "25     ham  Just forced myself to eat a slice. I'm really ...\n",
      "26     ham                     Lol your always so convincing.\n",
      "27     ham  Did you catch the bus ? Are you frying an egg ...\n",
      "28     ham  I'm back &amp; we're packing the car now, I'll...\n",
      "29     ham  Ahhh. Work. I vaguely remember that! What does...\n",
      "...    ...                                                ...\n",
      "5544   ham           Armand says get your ass over to epsilon\n",
      "5545   ham             U still havent got urself a jacket ah?\n",
      "5546   ham  I'm taking derek &amp; taylor to walmart, if I...\n",
      "5547   ham      Hi its in durban are you still on this number\n",
      "5548   ham         Ic. There are a lotta childporn cars then.\n",
      "5549  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
      "5550   ham                 No, I was trying it all weekend ;V\n",
      "5551   ham  You know, wot people wear. T shirts, jumpers, ...\n",
      "5552   ham        Cool, what time you think you can get here?\n",
      "5553   ham  Wen did you get so spiritual and deep. That's ...\n",
      "5554   ham  Have a safe trip to Nigeria. Wish you happines...\n",
      "5555   ham                        Hahaha..use your brain dear\n",
      "5556   ham  Well keep in mind I've only got enough gas for...\n",
      "5557   ham  Yeh. Indians was nice. Tho it did kane me off ...\n",
      "5558   ham  Yes i have. So that's why u texted. Pshew...mi...\n",
      "5559   ham  No. I meant the calculation is the same. That ...\n",
      "5560   ham                             Sorry, I'll call later\n",
      "5561   ham  if you aren't here in the next  &lt;#&gt;  hou...\n",
      "5562   ham                  Anything lor. Juz both of us lor.\n",
      "5563   ham  Get me out of this dump heap. My mom decided t...\n",
      "5564   ham  Ok lor... Sony ericsson salesman... I ask shuh...\n",
      "5565   ham                                Ard 6 like dat lor.\n",
      "5566   ham  Why don't you wait 'til at least wednesday to ...\n",
      "5567   ham                                       Huh y lei...\n",
      "5568  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
      "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5570   ham               Will ü b going to esplanade fr home?\n",
      "5571   ham  Pity, * was in mood for that. So...any other s...\n",
      "5572   ham  The guy did some bitching but I acted like i'd...\n",
      "5573   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5574 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "messages = pandas.read_csv('/data/cs2300/L5/SMSSpamCollection.txt', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should take a look at the basic statistics for this dataset using Pandas describe() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827</td>\n",
       "      <td>4518</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4827   4518                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add a Pandas column that describes the length of the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow you to run the cell below to make a histogram of the length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2694540410>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.length.plot(bins=20, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5574.000000\n",
       "mean       80.478292\n",
       "std        59.848302\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and print that really long one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['message'].iloc[messages['length'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see if there is there any difference in message length between spam and ham by running the following code to plot them side by side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f2694540190>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f26675c3d90>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, but this is not sufficient for us to create a classifier.  We need machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert the raw messages (sequence of characters) into vectors (sequences of numbers).\n",
    "\n",
    "The mapping is not 1-to-1; we'll use the [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model) approach, where each unique word in a text will be represented by one number.\n",
    "\n",
    "As a first step, here is a function that will split a message into its individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_tokens(message):\n",
    "    return TextBlob(message).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should tokenize them by applying the split_into_tokens method to the message column of the dataframe in the following cell.  Print the results to convince yourself that they are correct.  You do not need to store these results back in the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, until, jurong, point, crazy, Available, o...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
       "3    [U, dun, say, so, early, hor, U, c, already, t...\n",
       "4    [Nah, I, do, n't, think, he, goes, to, usf, he...\n",
       "5    [FreeMsg, Hey, there, darling, it, 's, been, 3...\n",
       "6    [Even, my, brother, is, not, like, to, speak, ...\n",
       "7    [As, per, your, request, 'Melle, Melle, Oru, M...\n",
       "8    [WINNER, As, a, valued, network, customer, you...\n",
       "9    [Had, your, mobile, 11, months, or, more, U, R...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_test = messages[\"message\"].map(lambda token: split_into_tokens(token))\n",
    "count_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With textblob, we can detect [part-of-speech (POS)](http://www.ling.upenn.edu/courses/Fall_2007/ling001/penn_treebank_pos.html) tags with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('world', 'NN'),\n",
       " ('how', 'WRB'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('going', 'VBG')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Hello world, how is it going?\").tags  # list of (word, POS) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize words into their base form ([lemmas](http://en.wikipedia.org/wiki/Lemmatisation)) by applying the split_into_lemmas function below to the message column of the dataframe.  Again, you do not need to store these results, so you can use `.head()` to view the output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, until, jurong, point, crazy, Available, o...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
       "3    [U, dun, say, so, early, hor, U, c, already, t...\n",
       "4    [Nah, I, do, n't, think, he, goes, to, usf, he...\n",
       "5    [FreeMsg, Hey, there, darling, it, 's, been, 3...\n",
       "6    [Even, my, brother, is, not, like, to, speak, ...\n",
       "7    [As, per, your, request, 'Melle, Melle, Oru, M...\n",
       "8    [WINNER, As, a, valued, network, customer, you...\n",
       "9    [Had, your, mobile, 11, months, or, more, U, R...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_test = messages[\"message\"].map(lambda token: split_into_lemmas(token))\n",
    "count_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably think of many more ways to improve the preprocessing: decoding HTML entities (those `&amp;` and `&lt;` we saw above); filtering out stop words (pronouns etc); adding more features, such as an word-in-all-caps indicator and so on.  So keep those in mind for later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
    "\n",
    "Doing that requires essentially three steps, in the bag-of-words model:\n",
    "\n",
    "1. counting how many times does a word occur in each message (term frequency)\n",
    "2. weighting the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "3. normalizing the vectors to unit length, to abstract from the original text length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector has as many dimensions as there are unique words in the SMS corpus.  We can count the number of unique words using the following cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11010\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used `scikit-learn` (`sklearn`), a powerful Python library for teaching machine learning. It contains a multitude of various methods and options.\n",
    "\n",
    "Let's take one text message and get its bag-of-words counts as a vector, putting to use our new `bow_transformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4189)\t2\n",
      "  (0, 4762)\t1\n",
      "  (0, 5363)\t1\n",
      "  (0, 6219)\t1\n",
      "  (0, 6243)\t1\n",
      "  (0, 7137)\t1\n",
      "  (0, 9280)\t2\n",
      "  (0, 9589)\t1\n",
      "  (0, 10054)\t1\n",
      "(1, 11010)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, nine unique words are in this message.  Two of them appear twice, the rest only once. \n",
    "\n",
    "Write some code in the next cell that identifies the words that appear twice.  You are encouraged to use the CountVectorizer's get_feature_names() method to make this easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "print((bow_transformer.get_feature_names())[4189])\n",
    "print((bow_transformer.get_feature_names())[9280])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words counts for the entire SMS corpus are a large, sparse matrix (generated using `bow_transformer.transform()` on the appropriate dataframe column).  In the following cell, calculate the sparsity using `.nnz` and the shape.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986699797000932\n"
     ]
    }
   ],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "messages_bow_nnz = messages_bow.nnz\n",
    "messages_bow_shape = messages_bow.shape\n",
    "sparsity = 1 - (messages_bow_nnz)/(messages_bow_shape[0]*messages_bow_shape[1])\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets see what the bow array looks like if we convert it to a \"dense\" array and print it out.  Lots of 0s right?  We can calculate the storage required by using `sys.getsizeof(python_array)` so please add that call to the following cell.  The numpy array requires a different size measurement because it stores the array outside of Python, so you can use `numpy_array.data.nbytes` to find its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "490958032\n",
      "652984\n"
     ]
    }
   ],
   "source": [
    "messages_array = messages_bow.toarray()\n",
    "print(messages_array)\n",
    "print(sys.getsizeof(messages_array))\n",
    "print(messages_bow.data.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term weighting and normalization can be done with [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf), using scikit-learn's `TfidfTransformer`, and we can apply it to the message we used above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10054)\t0.22510385070095637\n",
      "  (0, 9589)\t0.1955442748962185\n",
      "  (0, 9280)\t0.49597495370832545\n",
      "  (0, 7137)\t0.4269339327922034\n",
      "  (0, 6243)\t0.3100112284407115\n",
      "  (0, 6219)\t0.2913528957227454\n",
      "  (0, 5363)\t0.2860779240943588\n",
      "  (0, 4762)\t0.25892595706356525\n",
      "  (0, 4189)\t0.391088549792437\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the entire bag-of-words corpus into TF-IDF corpus at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 11010)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training a model, detecting spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With messages represented as vectors, we can finally train our spam/ham classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using scikit-learn here, choosing the [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detector = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try classifying our single random message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ham\n",
      "expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detector.predict(tfidf4)[0])\n",
    "print('expected:', messages.label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray!\n",
    "\n",
    "A natural question is to ask, how many messages do we classify correctly overall?  The following cell will calculate this for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9721923214926445\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detector.predict(messages_tfidf)\n",
    "print('accuracy', accuracy_score(messages['label'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few possible metrics for evaluating model performance. Which one is the most suitable depends on the task. For example, the cost of mispredicting \"spam\" as \"ham\" is probably much lower than mispredicting \"ham\" as \"spam\".  Differences between errors can be illuminated using metrics other than accuracy, so in the following cell, and in the cells below, you should use sklearn to calculate recall and precision in addition to accuracy.  Please include statements about what you can interpret from these results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Let's get realistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above \"evaluation\", we committed a cardinal sin. For simplicity of demonstration, we evaluated accuracy on the same data we used for training. **Never evaluate on the same dataset you train on!**\n",
    "\n",
    "Such evaluation tells us nothing about the true predictive power of our model. If we simply remembered each example during training, the accuracy on training data would trivially be 100%, even though we wouldn't be able to classify any new messages.  This is exactly like memorizing the exact answers for an exam without understanding the underlying material!\n",
    "\n",
    "A proper way is to split the data into a training/test set, where the model only ever sees the **training data** during its model fitting and parameter tuning. The **test data** is never used in any way -- thanks to this process, we make sure we are not \"cheating\", and that our final evaluation on test data is representative of true predictive performance.\n",
    "\n",
    "The following code splits the dataset into a training and testing set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459 1115 5574\n"
     ]
    }
   ],
   "source": [
    "msg_train, msg_test, label_train, label_test = \\\n",
    "    train_test_split(messages['message'], messages['label'], test_size=0.20)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as requested, the test size is 20% of the entire dataset.\n",
    "\n",
    "Next, lets set up our split datasets to be ready to be used by the Bayes model for training and prediction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages_bow = bow_transformer.transform(msg_train)\n",
    "train_tfidf_transformer = TfidfTransformer().fit(train_messages_bow)\n",
    "train_messages_tfidf = train_tfidf_transformer.transform(train_messages_bow)\n",
    "test_messages_bow = bow_transformer.transform(msg_test)\n",
    "test_tfidf_transformer = TfidfTransformer().fit(test_messages_bow)\n",
    "test_messages_tfidf = test_tfidf_transformer.transform(test_messages_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a new Naive Bayes classifier with only the training data, and test it with the test data, and our accuracy should drop.  In this cell answer: why? \n",
    "\n",
    "The accuracy should drop as the test data will most likely contain words not found in the training data. This makes logical sense as the model only knows what it has been taught. If XXX is a spam word, but the model hasn't been told that, then it can't be expected to know XXX is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated accuracy 0.9641255605381166\n"
     ]
    }
   ],
   "source": [
    "split_spam_detector = MultinomialNB().fit(train_messages_tfidf, label_train)\n",
    "test_predictions = split_spam_detector.predict(test_messages_tfidf)\n",
    "print('updated accuracy', accuracy_score(label_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, re-run this experiment changing the test size to a different value (in the subsequent cells of this part) and develop an explanation for the results (it should be different than your accuracy value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459 1115 5574\n",
      "updated accuracy 0.9506726457399103\n"
     ]
    }
   ],
   "source": [
    "msg_train, msg_test, label_train, label_test = \\\n",
    "    train_test_split(messages['message'], messages['label'], test_size=0.20)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))\n",
    "\n",
    "train_messages_bow = bow_transformer.transform(msg_train)\n",
    "train_tfidf_transformer = TfidfTransformer().fit(train_messages_bow)\n",
    "train_messages_tfidf = train_tfidf_transformer.transform(train_messages_bow)\n",
    "test_messages_bow = bow_transformer.transform(msg_test)\n",
    "test_tfidf_transformer = TfidfTransformer().fit(test_messages_bow)\n",
    "test_messages_tfidf = test_tfidf_transformer.transform(test_messages_bow)\n",
    "\n",
    "split_spam_detector = MultinomialNB().fit(train_messages_tfidf, label_train)\n",
    "test_predictions = split_spam_detector.predict(test_messages_tfidf)\n",
    "print('updated accuracy', accuracy_score(label_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4459, 11010)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_messages_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a test sample of 50% the accuracy was 93.94%, whereas the previous example of 20% had an accuracy rating of 95.96%. This very slight decrease of 2.02% is easily explained by the frequency of spam in the dataset and the model's lack of knowledge. The size of the decrease in accuracy is caused by the dataset having a low amount of spam. If the amount of spam is low then the change in accuracy would also be low. The decrease in accuracy is caused by the model's lack of knowledge. As was stated earlier, the model won't be able to identitfy every case of spam without knowing every case of spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Next Steps\n",
    "\n",
    "In the following cells you should make some changes to the dataset (cast to lowercase, remove numbers, remove non-words, add content, etc) to sufficiently change the sparsity percentage.  The number of columns in your bag of words model should be significantly smaller.  The goal of this is see the size comparison in the non-compressed version of the matrix (`toarray`) vs the sparse representation as the size of the data changes.  If we didn't have a sparse representation, our ability to use a BOW model would be very limiting...\n",
    "\n",
    "Run the experiments again to assess the accuracy of your new dataset and compare it with your previous results.  You should make arguments about what caused the changes and why they make sense.  Calculate and compare the storage requirements of the non-sparse and sparse representations, and argue how using sparse matricies can enable better accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes in a string and deletes it if it has less than 4 letters.\n",
    "\n",
    "**Param** word: string to be tested\n",
    "**Return**:A string that is either 0 in length or greater than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_string(word):\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "    return(shortword.sub('', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>until jurong point, crazy vailable only bugis...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>oking</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>entry wkly comp final tkts receive entry ques...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>early already then</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>think goes lives around here though</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham   until jurong point, crazy vailable only bugis...      77\n",
       "1   ham                                              oking       6\n",
       "2  spam   entry wkly comp final tkts receive entry ques...      69\n",
       "3   ham                                 early already then      19\n",
       "4   ham                think goes lives around here though      36"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#Data set with only lower case letters and words greater than 3 chars\n",
    "messages_mod = pandas.DataFrame()\n",
    "messages_mod[\"label\"] = messages[\"label\"]\n",
    "messages_mod[\"message\"] = messages[\"message\"].str.replace('[^a-z, \\']', '')\n",
    "messages_mod[\"message\"] = messages_mod[\"message\"].map(lambda token: remove_small_string(token))\n",
    "messages_mod[\"length\"] = messages_mod[\"message\"].str.len()\n",
    "messages_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459 1115 5574\n",
      "updated accuracy 0.9461883408071748\n"
     ]
    }
   ],
   "source": [
    "msg_train, msg_test, label_train, label_test = \\\n",
    "    train_test_split(messages_mod['message'], messages_mod['label'], test_size=0.20)\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))\n",
    "\n",
    "bow_transformer_mod = CountVectorizer(analyzer=split_into_lemmas).fit(messages_mod['message'])\n",
    "\n",
    "train_messages_bow = bow_transformer_mod.transform(msg_train)\n",
    "train_tfidf_transformer = TfidfTransformer().fit(train_messages_bow)\n",
    "train_messages_tfidf = train_tfidf_transformer.transform(train_messages_bow)\n",
    "test_messages_bow = bow_transformer_mod.transform(msg_test)\n",
    "test_tfidf_transformer = TfidfTransformer().fit(test_messages_bow)\n",
    "test_messages_tfidf = test_tfidf_transformer.transform(test_messages_bow)\n",
    "\n",
    "split_spam_detector = MultinomialNB().fit(train_messages_tfidf, label_train)\n",
    "test_predictions = split_spam_detector.predict(test_messages_tfidf)\n",
    "print('updated accuracy', accuracy_score(label_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990641173172639\n"
     ]
    }
   ],
   "source": [
    "#Sparsity Calculations\n",
    "messages_mod_bow = bow_transformer_mod.transform(messages_mod['message'])\n",
    "messages_mod_bow_nnz = messages_mod_bow.nnz\n",
    "messages_mod_bow_shape = messages_mod_bow.shape\n",
    "sparsity_mod = 1 - (messages_mod_bow_nnz)/(messages_mod_bow_shape[0]*messages_mod_bow_shape[1])\n",
    "print(sparsity_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 7068)\n",
      "315176368\n",
      "294968\n"
     ]
    }
   ],
   "source": [
    "#Size of the bag of words\n",
    "messages_mod_array = messages_mod_bow.toarray()\n",
    "print(messages_mod_bow.shape)\n",
    "print(sys.getsizeof(messages_mod_array))\n",
    "print(messages_mod_bow.data.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "**Accuracy**  \n",
    "The modified dataset at 20% test size had an accuracy rate difference of +0.90%. This increase in accuracy can be attributed to the smaller dataset. Most text lingo utilize short versions of words such as \"u\" for \"you\" and \"4\" for \"four\". By removing these type of words, the model would have to sift through less data, thus the probability of learning data increases due to the higher chance of spam related words being fed into the model.  \n",
    "\n",
    "**Sparcity**  \n",
    "The modified dataset had a difference in sparcity value of  +0.04%. This increase in sparcity can again be attributed to removing alphabetized words and words less than 3 chars in length. By removing captial letters, we effectively split the rate of occurance of 5+ char words between the lowercase and uppercase versions, thus increasing the amount of columns in the sparcity matrix.  \n",
    "\n",
    "**Bag of words**  \n",
    "The modified dataset had a difference in the number of columns in the bag of words of -35.80%, and a difference in size of -54.83%. This decrease in the bag of words size can be attributed to the total amount of words lost due to remove all non-lowercase chars and words with less than 3 chars. Less words in the data set means a smaller bag of words. Texting lingo typically contains simple words and use of numbers, meaning the likelyhood of using 3-letter words is higher than other forms of communication.  \n",
    "\n",
    "**Bag of words**  \n",
    "Comparing the size of the dense array to the sparse array for the bag of words, we can clearly see the incredible data compression sparcity allows. The sparse bag of words is 294,968 Bytes while the dense version is 315,176,368 Bytes; a difference of 99.91%. This dramatic size difference would also translate to faster calculations as less data needs to be fed into the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
